{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c54d5436-a103-4743-8731-13e8510a2d94",
      "metadata": {
        "id": "c54d5436-a103-4743-8731-13e8510a2d94"
      },
      "source": [
        "## Homework 9: Text Classification with Fine-Tuned BERT\n",
        "\n",
        "### Due: Midnight on November 5th (with 2-hour grace period) — Worth 85 points\n",
        "\n",
        "In this final homework, we’ll explore **fine-tuning a pre-trained Transformer model (BERT)** for text classification using the **IMDB Movie Review** dataset. You’ll begin with a working baseline notebook and then conduct a series of controlled experiments to understand how data size, context length, and model architecture affect performance.\n",
        "\n",
        "You’ll complete three problems:\n",
        "\n",
        "* **Problem 1:** Evaluate how **sequence length** and **learning rate** jointly influence validation loss and generalization.\n",
        "* **Problem 2:** Measure how **training data size** affects both model performance and total training time.\n",
        "* **Problem 3:** Compare **two additional models** from the BERT family to analyze the trade-offs between model size and accuracy on this dataset.\n",
        "\n",
        "In each problem, you’ll report your key metrics, summarize what you observed, and reflect on what you learned.\n",
        "\n",
        "> **Note:** This homework was developed and tested on **Google Colab**, due to version conflicts when running locally. It is **strongly recommended** that you complete your work on Colab as well.\n",
        "\n",
        "There are 6 problems, each worth 14 points, and you get one point free if you complete the entire homework.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "S-zVZFxlTWxP",
      "metadata": {
        "id": "S-zVZFxlTWxP"
      },
      "outputs": [],
      "source": [
        "# Install once per new Colab runtime\n",
        "%pip -q install -U keras keras-hub tensorflow tensorflow-text datasets evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "bb6035ef-078b-4ae3-8695-7e0d135cd51e",
      "metadata": {
        "id": "bb6035ef-078b-4ae3-8695-7e0d135cd51e"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import keras\n",
        "import keras_hub as kh\n",
        "import evaluate\n",
        "from datasets import load_dataset, Dataset, Features, Value, ClassLabel\n",
        "\n",
        "from keras import mixed_precision                    # generally faster\n",
        "mixed_precision.set_global_policy(\"mixed_float16\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50a9307f-eb34-49e7-84e8-ed373dbd8ef3",
      "metadata": {
        "id": "50a9307f-eb34-49e7-84e8-ed373dbd8ef3"
      },
      "source": [
        "### Here is where you can set global hyperparameters for this homework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "189c9bbc-c634-417f-909b-ccc4d5945c76",
      "metadata": {
        "id": "189c9bbc-c634-417f-909b-ccc4d5945c76"
      },
      "outputs": [],
      "source": [
        "# ---------------- Config ----------------\n",
        "SEED        = 42\n",
        "MAX_LEN     = 512\n",
        "EPOCHS      = 3\n",
        "BATCH       = 8\n",
        "EVAL_BATCH  = 64\n",
        "SUBSET_FRAC = 1.00   # <-- 0.25 to train and test on 25% of whole dataset during development;  set to 1.0 for full dataset\n",
        "\n",
        "keras.utils.set_random_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d33d68ff-94e2-402a-8bd3-fef3cba9e7bf",
      "metadata": {
        "id": "d33d68ff-94e2-402a-8bd3-fef3cba9e7bf"
      },
      "source": [
        "### Load and Preprocess the IMDB Movie Review Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "59c3b6bd-ddc3-4c75-b0ba-8b09d2f42a96",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59c3b6bd-ddc3-4c75-b0ba-8b09d2f42a96",
        "outputId": "429a7298-12ff-44ae-efbd-58f117e9ee77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pool after SUBSET_FRAC=0.75: 37500 (of 50000)\n",
            "Train: (26250, [13125, 13125])  Val: (3750, [1875, 1875])  Test: (7500, [3750, 3750])\n"
          ]
        }
      ],
      "source": [
        "# ---- Load IMDb (raw), join train+test ----\n",
        "imdb   = load_dataset(\"imdb\")\n",
        "texts  = list(imdb[\"train\"][\"text\"]) + list(imdb[\"test\"][\"text\"])\n",
        "labels = np.array(list(imdb[\"train\"][\"label\"]) + list(imdb[\"test\"][\"label\"]), dtype=\"int32\")\n",
        "\n",
        "# ---- Build DS with explicit features (label=ClassLabel) ----\n",
        "features = Features({\"text\": Value(\"string\"),\n",
        "                     \"label\": ClassLabel(num_classes=2, names=[\"NEG\",\"POS\"])})\n",
        "all_ds = Dataset.from_dict({\"text\": texts, \"label\": labels.tolist()}, features=features)\n",
        "\n",
        "# ---- Optional: take a stratified subset of the FULL dataset ----\n",
        "if 0.0 < SUBSET_FRAC < 1.0:\n",
        "    sub = all_ds.train_test_split(train_size=SUBSET_FRAC, seed=SEED, stratify_by_column=\"label\")\n",
        "    ds_pool = sub[\"train\"]\n",
        "else:\n",
        "    ds_pool = all_ds\n",
        "\n",
        "# ---- Stratified 80/10/10 split on the (possibly smaller) pool ----\n",
        "# First: 80/20 train+val pool / test\n",
        "splits = ds_pool.train_test_split(test_size=0.20, seed=SEED, stratify_by_column=\"label\")\n",
        "train_val_pool, test_ds = splits[\"train\"], splits[\"test\"]\n",
        "# Then: carve 10% of full (i.e., 0.125 of the 80% pool) as validation\n",
        "splits2 = train_val_pool.train_test_split(test_size=0.125, seed=SEED, stratify_by_column=\"label\")\n",
        "train_ds, val_ds = splits2[\"train\"], splits2[\"test\"]\n",
        "\n",
        "# ---- Numpy arrays for Keras fit/predict ----\n",
        "X_tr = np.array(train_ds[\"text\"], dtype=object); y_tr = np.array(train_ds[\"label\"], dtype=\"int32\")\n",
        "X_va = np.array(val_ds[\"text\"],   dtype=object); y_va = np.array(val_ds[\"label\"],   dtype=\"int32\")\n",
        "X_te = np.array(test_ds[\"text\"],  dtype=object); y_te = np.array(test_ds[\"label\"],  dtype=\"int32\")\n",
        "\n",
        "# ---- Quick summary ----\n",
        "def _counts(ds):\n",
        "    arr = np.array(ds[\"label\"], dtype=int)\n",
        "    return len(arr), np.bincount(arr, minlength=2).tolist()\n",
        "print(f\"Pool after SUBSET_FRAC={SUBSET_FRAC}: {len(ds_pool)} (of {len(all_ds)})\")\n",
        "print(\"Train:\", _counts(train_ds), \" Val:\", _counts(val_ds), \" Test:\", _counts(test_ds))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6e3cf44-614f-4b4b-801a-8865adf1ded3",
      "metadata": {
        "id": "c6e3cf44-614f-4b4b-801a-8865adf1ded3"
      },
      "source": [
        "### Build and train a baseline Distil-Bert Text Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "4814b089-9299-4828-865a-81ae45bb2bb5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4814b089-9299-4828-865a-81ae45bb2bb5",
        "outputId": "b3270b44-897e-4304-e261-ba0825ac5e5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 69ms/step - acc: 0.8306 - loss: 0.3757 - val_acc: 0.8675 - val_loss: 0.2999\n",
            "Epoch 2/3\n",
            "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 29ms/step - acc: 0.8890 - loss: 0.2678 - val_acc: 0.8779 - val_loss: 0.3087\n",
            "Epoch 3/3\n",
            "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 29ms/step - acc: 0.9164 - loss: 0.2112 - val_acc: 0.8715 - val_loss: 0.3529\n",
            "\n",
            "Validation acc (best epoch): 0.867\n",
            "\n",
            "Test accuracy: 0.868   Test F1: 0.869\n",
            "\n",
            "Confusion matrix:\n",
            " [[3234  516]\n",
            " [ 472 3278]]\n",
            "\n",
            "Elapsed time: 00:02:44\n"
          ]
        }
      ],
      "source": [
        "# ---- Keras Hub preprocessor + classifier ----\n",
        "preproc = kh.models.DistilBertTextClassifierPreprocessor.from_preset(\n",
        "    \"distil_bert_base_en_uncased\", sequence_length=MAX_LEN\n",
        ")\n",
        "model = kh.models.DistilBertTextClassifier.from_preset(\n",
        "    \"distil_bert_base_en_uncased\", num_classes=2, preprocessor=preproc\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-5),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
        ")\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# ---- Train with early stopping (restore best val weights) ----\n",
        "cb = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)]\n",
        "history = model.fit(\n",
        "    X_tr, y_tr,\n",
        "    validation_data=(X_va, y_va),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH,\n",
        "    callbacks=cb,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# ---- Evaluate (accuracy + F1 via `evaluate`) ----\n",
        "logits = model.predict(X_te, batch_size=EVAL_BATCH, verbose=0)\n",
        "y_pred = logits.argmax(axis=-1)\n",
        "\n",
        "acc_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric  = evaluate.load(\"f1\")\n",
        "acc = acc_metric.compute(predictions=y_pred, references=y_te)[\"accuracy\"]\n",
        "f1  = f1_metric.compute(predictions=y_pred, references=y_te)[\"f1\"]\n",
        "\n",
        "# Tiny confusion matrix helper (no sklearn needed)\n",
        "def confusion_matrix_np(y_true, y_pred, num_classes=2):\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
        "    for t, p in zip(y_true, y_pred):\n",
        "        cm[t, p] += 1\n",
        "    return cm\n",
        "\n",
        "print(f\"\\nValidation acc (best epoch): {history.history['val_acc'][np.argmin(history.history['val_loss'])]:.3f}\")\n",
        "print(f\"\\nTest accuracy: {acc:.3f}   Test F1: {f1:.3f}\")\n",
        "print(\"\\nConfusion matrix:\\n\", confusion_matrix_np(y_te, y_pred))\n",
        "\n",
        "end = time.time() - start\n",
        "print(\"\\nElapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(end)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2fe4979-adac-4d73-aa57-a91cd0b2ad34",
      "metadata": {
        "id": "d2fe4979-adac-4d73-aa57-a91cd0b2ad34"
      },
      "source": [
        "# Problem 1 — Mini sweep: context length × learning rate (6 runs)\n",
        "\n",
        "In this problem we'll see how much **context length** (`MAX_LEN`) helps, and how sensitive fine-tuning is to **learning rate**—without running a huge grid.\n",
        "\n",
        "## Setup (keep these fixed)\n",
        "\n",
        "* `SUBSET_FRAC = 0.25`               # use only this percentage of the whole dataset\n",
        "* `EPOCHS = 3`\n",
        "* `BATCH = 32` (but see note for 256 below)\n",
        "* **EarlyStopping** with `restore_best_weights=True`\n",
        "* Same random `SEED` for all runs\n",
        "* Same data split for all runs (don’t reshuffle between runs)\n",
        "\n",
        "### Run these 6 configurations\n",
        "\n",
        "**For each** `MAX_LEN ∈ {128, 256, 512}`, try **two** learning rates:\n",
        "\n",
        "* **MAX_LEN = 128**\n",
        "\n",
        "  * `(LR = 2e-5, BATCH = 32)` – healthy default for shorter contexts.\n",
        "  * `(LR = 1e-5, BATCH = 32)` – conservative LR; often a touch stabler.\n",
        "\n",
        "* **MAX_LEN = 256**\n",
        "\n",
        "  * `(LR = 1e-5, BATCH = 16)` – longer context → lower batch.\n",
        "  * `(LR = 7.5e-6, BATCH = 16)` – even steadier if loss is noisy.\n",
        "\n",
        "* **MAX_LEN = 512**  *(heavier quadratic attention cost)*\n",
        "\n",
        "  * `(LR = 7.5e-6, BATCH = 8)` – safe starting point.\n",
        "  * `(LR = 5e-6, BATCH = 8)` – extra caution for stability.\n",
        "\n",
        "**If you hit an Out Of Memory error:**\n",
        "\n",
        "* At **256** with `BATCH = 16`, drop to `BATCH = 8`.\n",
        "* At **512** with `BATCH = 8`, drop to `BATCH = 4`.\n",
        "\n",
        "\n",
        "Then answer the graded questions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MAX_LEN = 128 (LR = 2e-5, BATCH = 32)"
      ],
      "metadata": {
        "id": "kH3OjZ_iVFB5"
      },
      "id": "kH3OjZ_iVFB5"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "bd105932-3dc1-47e5-9b4a-90f4e9206143",
      "metadata": {
        "id": "bd105932-3dc1-47e5-9b4a-90f4e9206143",
        "outputId": "c651053b-e321-42fd-a7ea-1e3068a7a899",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 70ms/step - acc: 0.8380 - loss: 0.3586 - val_acc: 0.8680 - val_loss: 0.2953\n",
            "Epoch 2/3\n",
            "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 31ms/step - acc: 0.9026 - loss: 0.2413 - val_acc: 0.8757 - val_loss: 0.3092\n",
            "Epoch 3/3\n",
            "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 31ms/step - acc: 0.9390 - loss: 0.1622 - val_acc: 0.8701 - val_loss: 0.3842\n",
            "\n",
            "Validation acc (best epoch): 0.868\n",
            "\n",
            "Test accuracy: 0.867   Test F1: 0.873\n",
            "\n",
            "Confusion matrix:\n",
            " [[3060  690]\n",
            " [ 309 3441]]\n",
            "\n",
            "Elapsed time: 00:02:47\n"
          ]
        }
      ],
      "source": [
        "# Your code here; add as many cells as you need\n",
        "# ---- Keras Hub preprocessor + classifier ----\n",
        "preproc = kh.models.DistilBertTextClassifierPreprocessor.from_preset(\n",
        "    \"distil_bert_base_en_uncased\", sequence_length=MAX_LEN\n",
        ")\n",
        "model = kh.models.DistilBertTextClassifier.from_preset(\n",
        "    \"distil_bert_base_en_uncased\", num_classes=2, preprocessor=preproc\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(2e-5),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
        ")\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# ---- Train with early stopping (restore best val weights) ----\n",
        "cb = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)]\n",
        "history = model.fit(\n",
        "    X_tr, y_tr,\n",
        "    validation_data=(X_va, y_va),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH,\n",
        "    callbacks=cb,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# ---- Evaluate (accuracy + F1 via `evaluate`) ----\n",
        "logits = model.predict(X_te, batch_size=EVAL_BATCH, verbose=0)\n",
        "y_pred = logits.argmax(axis=-1)\n",
        "\n",
        "acc_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric  = evaluate.load(\"f1\")\n",
        "acc = acc_metric.compute(predictions=y_pred, references=y_te)[\"accuracy\"]\n",
        "f1  = f1_metric.compute(predictions=y_pred, references=y_te)[\"f1\"]\n",
        "\n",
        "# Tiny confusion matrix helper (no sklearn needed)\n",
        "def confusion_matrix_np(y_true, y_pred, num_classes=2):\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
        "    for t, p in zip(y_true, y_pred):\n",
        "        cm[t, p] += 1\n",
        "    return cm\n",
        "\n",
        "print(f\"\\nValidation acc (best epoch): {history.history['val_acc'][np.argmin(history.history['val_loss'])]:.3f}\")\n",
        "print(f\"\\nTest accuracy: {acc:.3f}   Test F1: {f1:.3f}\")\n",
        "print(\"\\nConfusion matrix:\\n\", confusion_matrix_np(y_te, y_pred))\n",
        "\n",
        "end = time.time() - start\n",
        "print(\"\\nElapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(end)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MAX_LEN = 128 (LR = 1e-5, BATCH = 32)"
      ],
      "metadata": {
        "id": "RW1V5bITWQn-"
      },
      "id": "RW1V5bITWQn-"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Keras Hub preprocessor + classifier ----\n",
        "preproc = kh.models.DistilBertTextClassifierPreprocessor.from_preset(\n",
        "    \"distil_bert_base_en_uncased\", sequence_length=MAX_LEN\n",
        ")\n",
        "model = kh.models.DistilBertTextClassifier.from_preset(\n",
        "    \"distil_bert_base_en_uncased\", num_classes=2, preprocessor=preproc\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-5),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
        ")\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# ---- Train with early stopping (restore best val weights) ----\n",
        "cb = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)]\n",
        "history = model.fit(\n",
        "    X_tr, y_tr,\n",
        "    validation_data=(X_va, y_va),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH,\n",
        "    callbacks=cb,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# ---- Evaluate (accuracy + F1 via `evaluate`) ----\n",
        "logits = model.predict(X_te, batch_size=EVAL_BATCH, verbose=0)\n",
        "y_pred = logits.argmax(axis=-1)\n",
        "\n",
        "acc_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric  = evaluate.load(\"f1\")\n",
        "acc = acc_metric.compute(predictions=y_pred, references=y_te)[\"accuracy\"]\n",
        "f1  = f1_metric.compute(predictions=y_pred, references=y_te)[\"f1\"]\n",
        "\n",
        "# Tiny confusion matrix helper (no sklearn needed)\n",
        "def confusion_matrix_np(y_true, y_pred, num_classes=2):\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
        "    for t, p in zip(y_true, y_pred):\n",
        "        cm[t, p] += 1\n",
        "    return cm\n",
        "\n",
        "print(f\"\\nValidation acc (best epoch): {history.history['val_acc'][np.argmin(history.history['val_loss'])]:.3f}\")\n",
        "print(f\"\\nTest accuracy: {acc:.3f}   Test F1: {f1:.3f}\")\n",
        "print(\"\\nConfusion matrix:\\n\", confusion_matrix_np(y_te, y_pred))\n",
        "\n",
        "end = time.time() - start\n",
        "print(\"\\nElapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(end)))"
      ],
      "metadata": {
        "id": "DUuwN9HOWXA9",
        "outputId": "0674dc08-e4ee-42ee-b74b-3ebb44331eb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DUuwN9HOWXA9",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 71ms/step - acc: 0.8325 - loss: 0.3736 - val_acc: 0.8672 - val_loss: 0.3048\n",
            "Epoch 2/3\n",
            "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 31ms/step - acc: 0.8877 - loss: 0.2717 - val_acc: 0.8739 - val_loss: 0.3153\n",
            "Epoch 3/3\n",
            "\u001b[1m821/821\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 31ms/step - acc: 0.9183 - loss: 0.2103 - val_acc: 0.8771 - val_loss: 0.3583\n",
            "\n",
            "Validation acc (best epoch): 0.867\n",
            "\n",
            "Test accuracy: 0.864   Test F1: 0.867\n",
            "\n",
            "Confusion matrix:\n",
            " [[3170  580]\n",
            " [ 438 3312]]\n",
            "\n",
            "Elapsed time: 00:02:50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MAX_LEN = 256 (LR = 1e-5, BATCH = 16)"
      ],
      "metadata": {
        "id": "dMj_kSl2XOa3"
      },
      "id": "dMj_kSl2XOa3"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Keras Hub preprocessor + classifier ----\n",
        "preproc = kh.models.DistilBertTextClassifierPreprocessor.from_preset(\n",
        "    \"distil_bert_base_en_uncased\", sequence_length=MAX_LEN\n",
        ")\n",
        "model = kh.models.DistilBertTextClassifier.from_preset(\n",
        "    \"distil_bert_base_en_uncased\", num_classes=2, preprocessor=preproc\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-5),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
        ")\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# ---- Train with early stopping (restore best val weights) ----\n",
        "cb = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)]\n",
        "history = model.fit(\n",
        "    X_tr, y_tr,\n",
        "    validation_data=(X_va, y_va),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH,\n",
        "    callbacks=cb,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# ---- Evaluate (accuracy + F1 via `evaluate`) ----\n",
        "logits = model.predict(X_te, batch_size=EVAL_BATCH, verbose=0)\n",
        "y_pred = logits.argmax(axis=-1)\n",
        "\n",
        "acc_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric  = evaluate.load(\"f1\")\n",
        "acc = acc_metric.compute(predictions=y_pred, references=y_te)[\"accuracy\"]\n",
        "f1  = f1_metric.compute(predictions=y_pred, references=y_te)[\"f1\"]\n",
        "\n",
        "# Tiny confusion matrix helper (no sklearn needed)\n",
        "def confusion_matrix_np(y_true, y_pred, num_classes=2):\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
        "    for t, p in zip(y_true, y_pred):\n",
        "        cm[t, p] += 1\n",
        "    return cm\n",
        "\n",
        "print(f\"\\nValidation acc (best epoch): {history.history['val_acc'][np.argmin(history.history['val_loss'])]:.3f}\")\n",
        "print(f\"\\nTest accuracy: {acc:.3f}   Test F1: {f1:.3f}\")\n",
        "print(\"\\nConfusion matrix:\\n\", confusion_matrix_np(y_te, y_pred))\n",
        "\n",
        "end = time.time() - start\n",
        "print(\"\\nElapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(end)))"
      ],
      "metadata": {
        "id": "lai7qd8sXSOz",
        "outputId": "048d75ae-d5d5-422b-da93-66eb54f21161",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "lai7qd8sXSOz",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m1641/1641\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 47ms/step - acc: 0.8746 - loss: 0.2944 - val_acc: 0.9107 - val_loss: 0.2254\n",
            "Epoch 2/3\n",
            "\u001b[1m1641/1641\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 26ms/step - acc: 0.9282 - loss: 0.1904 - val_acc: 0.9053 - val_loss: 0.2461\n",
            "Epoch 3/3\n",
            "\u001b[1m1641/1641\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 27ms/step - acc: 0.9529 - loss: 0.1344 - val_acc: 0.9043 - val_loss: 0.2731\n",
            "\n",
            "Validation acc (best epoch): 0.911\n",
            "\n",
            "Test accuracy: 0.909   Test F1: 0.911\n",
            "\n",
            "Confusion matrix:\n",
            " [[3341  409]\n",
            " [ 274 3476]]\n",
            "\n",
            "Elapsed time: 00:03:51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MAX_LEN = 256 (LR = 7.5e-6, BATCH = 16)"
      ],
      "metadata": {
        "id": "KQW6juf_YxJC"
      },
      "id": "KQW6juf_YxJC"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Keras Hub preprocessor + classifier ----\n",
        "preproc = kh.models.DistilBertTextClassifierPreprocessor.from_preset(\n",
        "    \"distil_bert_base_en_uncased\", sequence_length=MAX_LEN\n",
        ")\n",
        "model = kh.models.DistilBertTextClassifier.from_preset(\n",
        "    \"distil_bert_base_en_uncased\", num_classes=2, preprocessor=preproc\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(7.5e-6),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
        ")\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# ---- Train with early stopping (restore best val weights) ----\n",
        "cb = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)]\n",
        "history = model.fit(\n",
        "    X_tr, y_tr,\n",
        "    validation_data=(X_va, y_va),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH,\n",
        "    callbacks=cb,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# ---- Evaluate (accuracy + F1 via `evaluate`) ----\n",
        "logits = model.predict(X_te, batch_size=EVAL_BATCH, verbose=0)\n",
        "y_pred = logits.argmax(axis=-1)\n",
        "\n",
        "acc_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric  = evaluate.load(\"f1\")\n",
        "acc = acc_metric.compute(predictions=y_pred, references=y_te)[\"accuracy\"]\n",
        "f1  = f1_metric.compute(predictions=y_pred, references=y_te)[\"f1\"]\n",
        "\n",
        "# Tiny confusion matrix helper (no sklearn needed)\n",
        "def confusion_matrix_np(y_true, y_pred, num_classes=2):\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
        "    for t, p in zip(y_true, y_pred):\n",
        "        cm[t, p] += 1\n",
        "    return cm\n",
        "\n",
        "print(f\"\\nValidation acc (best epoch): {history.history['val_acc'][np.argmin(history.history['val_loss'])]:.3f}\")\n",
        "print(f\"\\nTest accuracy: {acc:.3f}   Test F1: {f1:.3f}\")\n",
        "print(\"\\nConfusion matrix:\\n\", confusion_matrix_np(y_te, y_pred))\n",
        "\n",
        "end = time.time() - start\n",
        "print(\"\\nElapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(end)))"
      ],
      "metadata": {
        "id": "rCrrb17dY3T5",
        "outputId": "41f64b30-e5c7-4b07-f1b7-8f4961355728",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "rCrrb17dY3T5",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m1641/1641\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 47ms/step - acc: 0.8690 - loss: 0.3060 - val_acc: 0.9029 - val_loss: 0.2361\n",
            "Epoch 2/3\n",
            "\u001b[1m1641/1641\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 27ms/step - acc: 0.9229 - loss: 0.2036 - val_acc: 0.9072 - val_loss: 0.2395\n",
            "Epoch 3/3\n",
            "\u001b[1m1641/1641\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 27ms/step - acc: 0.9427 - loss: 0.1560 - val_acc: 0.9043 - val_loss: 0.2686\n",
            "\n",
            "Validation acc (best epoch): 0.903\n",
            "\n",
            "Test accuracy: 0.909   Test F1: 0.910\n",
            "\n",
            "Confusion matrix:\n",
            " [[3364  386]\n",
            " [ 299 3451]]\n",
            "\n",
            "Elapsed time: 00:03:48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MAX_LEN = 512 (LR = 7.5e-6, BATCH = 8)"
      ],
      "metadata": {
        "id": "mZmf5tlmZ02K"
      },
      "id": "mZmf5tlmZ02K"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Keras Hub preprocessor + classifier ----\n",
        "preproc = kh.models.DistilBertTextClassifierPreprocessor.from_preset(\n",
        "    \"distil_bert_base_en_uncased\", sequence_length=MAX_LEN\n",
        ")\n",
        "model = kh.models.DistilBertTextClassifier.from_preset(\n",
        "    \"distil_bert_base_en_uncased\", num_classes=2, preprocessor=preproc\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(7.5e-6),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
        ")\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# ---- Train with early stopping (restore best val weights) ----\n",
        "cb = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)]\n",
        "history = model.fit(\n",
        "    X_tr, y_tr,\n",
        "    validation_data=(X_va, y_va),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH,\n",
        "    callbacks=cb,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# ---- Evaluate (accuracy + F1 via `evaluate`) ----\n",
        "logits = model.predict(X_te, batch_size=EVAL_BATCH, verbose=0)\n",
        "y_pred = logits.argmax(axis=-1)\n",
        "\n",
        "acc_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric  = evaluate.load(\"f1\")\n",
        "acc = acc_metric.compute(predictions=y_pred, references=y_te)[\"accuracy\"]\n",
        "f1  = f1_metric.compute(predictions=y_pred, references=y_te)[\"f1\"]\n",
        "\n",
        "# Tiny confusion matrix helper (no sklearn needed)\n",
        "def confusion_matrix_np(y_true, y_pred, num_classes=2):\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
        "    for t, p in zip(y_true, y_pred):\n",
        "        cm[t, p] += 1\n",
        "    return cm\n",
        "\n",
        "print(f\"\\nValidation acc (best epoch): {history.history['val_acc'][np.argmin(history.history['val_loss'])]:.3f}\")\n",
        "print(f\"\\nTest accuracy: {acc:.3f}   Test F1: {f1:.3f}\")\n",
        "print(\"\\nConfusion matrix:\\n\", confusion_matrix_np(y_te, y_pred))\n",
        "\n",
        "end = time.time() - start\n",
        "print(\"\\nElapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(end)))"
      ],
      "metadata": {
        "id": "3_S5Nf-0Z5Tl",
        "outputId": "af1002ef-60bc-4497-ced9-c3080bc5cdb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3_S5Nf-0Z5Tl",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m3282/3282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 39ms/step - acc: 0.8938 - loss: 0.2588 - val_acc: 0.9277 - val_loss: 0.1908\n",
            "Epoch 2/3\n",
            "\u001b[1m3282/3282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 29ms/step - acc: 0.9402 - loss: 0.1621 - val_acc: 0.9301 - val_loss: 0.1937\n",
            "Epoch 3/3\n",
            "\u001b[1m3282/3282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 29ms/step - acc: 0.9630 - loss: 0.1093 - val_acc: 0.9320 - val_loss: 0.2089\n",
            "\n",
            "Validation acc (best epoch): 0.928\n",
            "\n",
            "Test accuracy: 0.923   Test F1: 0.923\n",
            "\n",
            "Confusion matrix:\n",
            " [[3463  287]\n",
            " [ 288 3462]]\n",
            "\n",
            "Elapsed time: 00:06:25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MAX_LEN = 512 (LR = 5e-6, BATCH = 8)"
      ],
      "metadata": {
        "id": "C-ZExwjCgQ5w"
      },
      "id": "C-ZExwjCgQ5w"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Keras Hub preprocessor + classifier ----\n",
        "preproc = kh.models.DistilBertTextClassifierPreprocessor.from_preset(\n",
        "    \"distil_bert_base_en_uncased\", sequence_length=MAX_LEN\n",
        ")\n",
        "model = kh.models.DistilBertTextClassifier.from_preset(\n",
        "    \"distil_bert_base_en_uncased\", num_classes=2, preprocessor=preproc\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(5e-6),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
        ")\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# ---- Train with early stopping (restore best val weights) ----\n",
        "cb = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)]\n",
        "history = model.fit(\n",
        "    X_tr, y_tr,\n",
        "    validation_data=(X_va, y_va),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH,\n",
        "    callbacks=cb,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# ---- Evaluate (accuracy + F1 via `evaluate`) ----\n",
        "logits = model.predict(X_te, batch_size=EVAL_BATCH, verbose=0)\n",
        "y_pred = logits.argmax(axis=-1)\n",
        "\n",
        "acc_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric  = evaluate.load(\"f1\")\n",
        "acc = acc_metric.compute(predictions=y_pred, references=y_te)[\"accuracy\"]\n",
        "f1  = f1_metric.compute(predictions=y_pred, references=y_te)[\"f1\"]\n",
        "\n",
        "# Tiny confusion matrix helper (no sklearn needed)\n",
        "def confusion_matrix_np(y_true, y_pred, num_classes=2):\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
        "    for t, p in zip(y_true, y_pred):\n",
        "        cm[t, p] += 1\n",
        "    return cm\n",
        "\n",
        "print(f\"\\nValidation acc (best epoch): {history.history['val_acc'][np.argmin(history.history['val_loss'])]:.3f}\")\n",
        "print(f\"\\nTest accuracy: {acc:.3f}   Test F1: {f1:.3f}\")\n",
        "print(\"\\nConfusion matrix:\\n\", confusion_matrix_np(y_te, y_pred))\n",
        "\n",
        "end = time.time() - start\n",
        "print(\"\\nElapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(end)))"
      ],
      "metadata": {
        "id": "eTLpHb0lgU38",
        "outputId": "a8bc811b-5b76-4de1-f28f-3bdde2c2cec0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "eTLpHb0lgU38",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/distil_bert/keras/distil_bert_base_en_uncased/3/download/tokenizer.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 794/794 [00:00<00:00, 1.66MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/distil_bert/keras/distil_bert_base_en_uncased/3/download/config.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 462/462 [00:00<00:00, 841kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m3282/3282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 37ms/step - acc: 0.8898 - loss: 0.2691 - val_acc: 0.9221 - val_loss: 0.2015\n",
            "Epoch 2/3\n",
            "\u001b[1m3282/3282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 28ms/step - acc: 0.9363 - loss: 0.1731 - val_acc: 0.9269 - val_loss: 0.2058\n",
            "Epoch 3/3\n",
            "\u001b[1m3282/3282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 28ms/step - acc: 0.9568 - loss: 0.1271 - val_acc: 0.9296 - val_loss: 0.2251\n",
            "\n",
            "Validation acc (best epoch): 0.922\n",
            "\n",
            "Test accuracy: 0.921   Test F1: 0.920\n",
            "\n",
            "Confusion matrix:\n",
            " [[3484  266]\n",
            " [ 327 3423]]\n",
            "\n",
            "Elapsed time: 00:06:11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1aa6ea8a-6482-4c7b-81ba-aadbb3bfe564",
      "metadata": {
        "id": "1aa6ea8a-6482-4c7b-81ba-aadbb3bfe564"
      },
      "source": [
        "### Graded Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "a67d26c5-8294-438a-b29f-6a091180a1f8",
      "metadata": {
        "id": "a67d26c5-8294-438a-b29f-6a091180a1f8"
      },
      "outputs": [],
      "source": [
        "# Set a1a to the validation accuracy at min validation loss for your best configuration found in this problem\n",
        "\n",
        "a1a = 0.928             # Replace 0.0 with your answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "0dd1c33c-4761-4954-a04d-e4e4def945ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dd1c33c-4761-4954-a04d-e4e4def945ab",
        "outputId": "f0617f8f-659b-4573-d7bb-576004367c54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a1a = 0.9280\n"
          ]
        }
      ],
      "source": [
        "# Graded Answer\n",
        "# DO NOT change this cell in any way\n",
        "\n",
        "print(f'a1a = {a1a:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d46d2a89-301d-4f8d-9236-f88a1bc023ca",
      "metadata": {
        "id": "d46d2a89-301d-4f8d-9236-f88a1bc023ca"
      },
      "source": [
        "#### Question a1b:\n",
        "\n",
        "* Does **more context** (128 → 256 → 512) consistently help?\n",
        "* How much effect did the learning rate have on the validation accuracy?\n",
        "\n",
        "\n",
        "#### Your Answer Here:\n",
        "\n",
        "Having more context did consistently improve validation accuracy. On the other hand, learning rate only had minimal effect on validation accuracy, typically only changing the value by around 0.002-0.005."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ef49299-6f39-4b66-9e16-52417f3f917f",
      "metadata": {
        "id": "6ef49299-6f39-4b66-9e16-52417f3f917f"
      },
      "source": [
        "## Problem 2 — How much data is enough?\n",
        "\n",
        "In this problem, you’ll investigate how model performance scales with dataset size.\n",
        "\n",
        "**Setup.**\n",
        "Use the best `MAX_LEN` and `LR` values you found in **Problem 1**.\n",
        "\n",
        "**What to do:**\n",
        "\n",
        "1. For each value of `SUBSET_FRAC ∈ {0.25, 0.50, 0.75, 1.00}`, train your model once and observe the displayed performance metrics.\n",
        "2. Answer the discussion question below.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SUBSET_FRAC = 0.50"
      ],
      "metadata": {
        "id": "FYPi4tpxiAyh"
      },
      "id": "FYPi4tpxiAyh"
    },
    {
      "cell_type": "code",
      "source": [
        "SUBSET_FRAC = 0.50"
      ],
      "metadata": {
        "id": "Nhog96enIM3B"
      },
      "id": "Nhog96enIM3B",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "82b9ebc5-809a-4e13-a4c6-1325b0d9a171",
      "metadata": {
        "id": "82b9ebc5-809a-4e13-a4c6-1325b0d9a171",
        "outputId": "a1cfaf4c-bc22-43e9-b698-a89929a2051b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m3282/3282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 38ms/step - acc: 0.8957 - loss: 0.2570 - val_acc: 0.9264 - val_loss: 0.1954\n",
            "Epoch 2/3\n",
            "\u001b[1m3282/3282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 27ms/step - acc: 0.9410 - loss: 0.1599 - val_acc: 0.9288 - val_loss: 0.2146\n",
            "Epoch 3/3\n",
            "\u001b[1m3282/3282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 27ms/step - acc: 0.9637 - loss: 0.1080 - val_acc: 0.9301 - val_loss: 0.2233\n",
            "\n",
            "Validation acc (best epoch): 0.926\n",
            "\n",
            "Test accuracy: 0.922   Test F1: 0.922\n",
            "\n",
            "Confusion matrix:\n",
            " [[3493  257]\n",
            " [ 326 3424]]\n",
            "\n",
            "Elapsed time: 00:06:10\n"
          ]
        }
      ],
      "source": [
        "# Your code here; add as many cells as you need\n",
        "# ---- Keras Hub preprocessor + classifier ----\n",
        "preproc = kh.models.DistilBertTextClassifierPreprocessor.from_preset(\n",
        "    \"distil_bert_base_en_uncased\", sequence_length=MAX_LEN\n",
        ")\n",
        "model = kh.models.DistilBertTextClassifier.from_preset(\n",
        "    \"distil_bert_base_en_uncased\", num_classes=2, preprocessor=preproc\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(7.5e-6),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
        ")\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# ---- Train with early stopping (restore best val weights) ----\n",
        "cb = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)]\n",
        "history = model.fit(\n",
        "    X_tr, y_tr,\n",
        "    validation_data=(X_va, y_va),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH,\n",
        "    callbacks=cb,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# ---- Evaluate (accuracy + F1 via `evaluate`) ----\n",
        "logits = model.predict(X_te, batch_size=EVAL_BATCH, verbose=0)\n",
        "y_pred = logits.argmax(axis=-1)\n",
        "\n",
        "acc_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric  = evaluate.load(\"f1\")\n",
        "acc = acc_metric.compute(predictions=y_pred, references=y_te)[\"accuracy\"]\n",
        "f1  = f1_metric.compute(predictions=y_pred, references=y_te)[\"f1\"]\n",
        "\n",
        "# Tiny confusion matrix helper (no sklearn needed)\n",
        "def confusion_matrix_np(y_true, y_pred, num_classes=2):\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
        "    for t, p in zip(y_true, y_pred):\n",
        "        cm[t, p] += 1\n",
        "    return cm\n",
        "\n",
        "print(f\"\\nValidation acc (best epoch): {history.history['val_acc'][np.argmin(history.history['val_loss'])]:.3f}\")\n",
        "print(f\"\\nTest accuracy: {acc:.3f}   Test F1: {f1:.3f}\")\n",
        "print(\"\\nConfusion matrix:\\n\", confusion_matrix_np(y_te, y_pred))\n",
        "\n",
        "end = time.time() - start\n",
        "print(\"\\nElapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(end)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SUBSET_FRAC = 0.75"
      ],
      "metadata": {
        "id": "u0-wHiLdjHBI"
      },
      "id": "u0-wHiLdjHBI"
    },
    {
      "cell_type": "code",
      "source": [
        "SUBSET_FRAC = 0.75"
      ],
      "metadata": {
        "id": "Nvq5Phi3LB-E"
      },
      "id": "Nvq5Phi3LB-E",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Keras Hub preprocessor + classifier ----\n",
        "preproc = kh.models.DistilBertTextClassifierPreprocessor.from_preset(\n",
        "    \"distil_bert_base_en_uncased\", sequence_length=MAX_LEN\n",
        ")\n",
        "model = kh.models.DistilBertTextClassifier.from_preset(\n",
        "    \"distil_bert_base_en_uncased\", num_classes=2, preprocessor=preproc\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(7.5e-6),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
        ")\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# ---- Train with early stopping (restore best val weights) ----\n",
        "cb = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)]\n",
        "history = model.fit(\n",
        "    X_tr, y_tr,\n",
        "    validation_data=(X_va, y_va),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH,\n",
        "    callbacks=cb,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# ---- Evaluate (accuracy + F1 via `evaluate`) ----\n",
        "logits = model.predict(X_te, batch_size=EVAL_BATCH, verbose=0)\n",
        "y_pred = logits.argmax(axis=-1)\n",
        "\n",
        "acc_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric  = evaluate.load(\"f1\")\n",
        "acc = acc_metric.compute(predictions=y_pred, references=y_te)[\"accuracy\"]\n",
        "f1  = f1_metric.compute(predictions=y_pred, references=y_te)[\"f1\"]\n",
        "\n",
        "# Tiny confusion matrix helper (no sklearn needed)\n",
        "def confusion_matrix_np(y_true, y_pred, num_classes=2):\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
        "    for t, p in zip(y_true, y_pred):\n",
        "        cm[t, p] += 1\n",
        "    return cm\n",
        "\n",
        "print(f\"\\nValidation acc (best epoch): {history.history['val_acc'][np.argmin(history.history['val_loss'])]:.3f}\")\n",
        "print(f\"\\nTest accuracy: {acc:.3f}   Test F1: {f1:.3f}\")\n",
        "print(\"\\nConfusion matrix:\\n\", confusion_matrix_np(y_te, y_pred))\n",
        "\n",
        "end = time.time() - start\n",
        "print(\"\\nElapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(end)))"
      ],
      "metadata": {
        "id": "C0NPYsl_jO5O",
        "outputId": "c27b9015-3004-4f83-f45b-cbeab7cf2e19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "C0NPYsl_jO5O",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m3282/3282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 37ms/step - acc: 0.8928 - loss: 0.2575 - val_acc: 0.9280 - val_loss: 0.1966\n",
            "Epoch 2/3\n",
            "\u001b[1m3282/3282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 27ms/step - acc: 0.9418 - loss: 0.1616 - val_acc: 0.9264 - val_loss: 0.2055\n",
            "Epoch 3/3\n",
            "\u001b[1m3282/3282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 27ms/step - acc: 0.9621 - loss: 0.1086 - val_acc: 0.9291 - val_loss: 0.2199\n",
            "\n",
            "Validation acc (best epoch): 0.928\n",
            "\n",
            "Test accuracy: 0.924   Test F1: 0.924\n",
            "\n",
            "Confusion matrix:\n",
            " [[3493  257]\n",
            " [ 312 3438]]\n",
            "\n",
            "Elapsed time: 00:06:01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SUBSET_FRAC = 1.00"
      ],
      "metadata": {
        "id": "xtirQ7A8jPuI"
      },
      "id": "xtirQ7A8jPuI"
    },
    {
      "cell_type": "code",
      "source": [
        "SUBSET_FRAC = 1.00"
      ],
      "metadata": {
        "id": "GeaVrCagjVQn"
      },
      "id": "GeaVrCagjVQn",
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Keras Hub preprocessor + classifier ----\n",
        "preproc = kh.models.DistilBertTextClassifierPreprocessor.from_preset(\n",
        "    \"distil_bert_base_en_uncased\", sequence_length=MAX_LEN\n",
        ")\n",
        "model = kh.models.DistilBertTextClassifier.from_preset(\n",
        "    \"distil_bert_base_en_uncased\", num_classes=2, preprocessor=preproc\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(7.5e-6),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
        ")\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# ---- Train with early stopping (restore best val weights) ----\n",
        "cb = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)]\n",
        "history = model.fit(\n",
        "    X_tr, y_tr,\n",
        "    validation_data=(X_va, y_va),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH,\n",
        "    callbacks=cb,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# ---- Evaluate (accuracy + F1 via `evaluate`) ----\n",
        "logits = model.predict(X_te, batch_size=EVAL_BATCH, verbose=0)\n",
        "y_pred = logits.argmax(axis=-1)\n",
        "\n",
        "acc_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric  = evaluate.load(\"f1\")\n",
        "acc = acc_metric.compute(predictions=y_pred, references=y_te)[\"accuracy\"]\n",
        "f1  = f1_metric.compute(predictions=y_pred, references=y_te)[\"f1\"]\n",
        "\n",
        "# Tiny confusion matrix helper (no sklearn needed)\n",
        "def confusion_matrix_np(y_true, y_pred, num_classes=2):\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
        "    for t, p in zip(y_true, y_pred):\n",
        "        cm[t, p] += 1\n",
        "    return cm\n",
        "\n",
        "print(f\"\\nValidation acc (best epoch): {history.history['val_acc'][np.argmin(history.history['val_loss'])]:.3f}\")\n",
        "print(f\"\\nTest accuracy: {acc:.3f}   Test F1: {f1:.3f}\")\n",
        "print(\"\\nConfusion matrix:\\n\", confusion_matrix_np(y_te, y_pred))\n",
        "\n",
        "end = time.time() - start\n",
        "print(\"\\nElapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(end)))"
      ],
      "metadata": {
        "id": "3AidjpDAjRED",
        "outputId": "bb47d566-6530-4990-f1a5-302b874982df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3AidjpDAjRED",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m3282/3282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 37ms/step - acc: 0.8954 - loss: 0.2567 - val_acc: 0.9285 - val_loss: 0.1984\n",
            "Epoch 2/3\n",
            "\u001b[1m3282/3282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 27ms/step - acc: 0.9421 - loss: 0.1605 - val_acc: 0.9296 - val_loss: 0.2006\n",
            "Epoch 3/3\n",
            "\u001b[1m3282/3282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 27ms/step - acc: 0.9631 - loss: 0.1082 - val_acc: 0.9224 - val_loss: 0.2529\n",
            "\n",
            "Validation acc (best epoch): 0.929\n",
            "\n",
            "Test accuracy: 0.925   Test F1: 0.924\n",
            "\n",
            "Confusion matrix:\n",
            " [[3508  242]\n",
            " [ 321 3429]]\n",
            "\n",
            "Elapsed time: 00:06:04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43868b2d-3a63-4422-a11c-b0eb7ada89a7",
      "metadata": {
        "id": "43868b2d-3a63-4422-a11c-b0eb7ada89a7"
      },
      "source": [
        "### Graded Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "00abc6af-69f5-494e-8349-9e0ebf115e42",
      "metadata": {
        "id": "00abc6af-69f5-494e-8349-9e0ebf115e42"
      },
      "outputs": [],
      "source": [
        "# Set a2a to the validation accuracy at min validation loss for your best configuration found in this problem\n",
        "# (Yes, it is probably at 1.0!)\n",
        "\n",
        "a2a = 0.929             # Replace 0.0 with your answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "3a4afa0b-5e7d-4ba1-8bb5-8d5229aef4fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a4afa0b-5e7d-4ba1-8bb5-8d5229aef4fd",
        "outputId": "a8fdc1b6-149c-4c61-9967-3d103d5120f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a2a = 0.9290\n"
          ]
        }
      ],
      "source": [
        "# Graded Answer\n",
        "# DO NOT change this cell in any way\n",
        "\n",
        "print(f'a2a = {a2a:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ba14dbe-08bd-4a03-ad90-2e2046a9ac78",
      "metadata": {
        "id": "5ba14dbe-08bd-4a03-ad90-2e2046a9ac78"
      },
      "source": [
        "#### Question a2b:\n",
        "\n",
        "Summarize what you observed as dataset size increased. Given that validation metrics are typically reliable to only about two decimal places, do the performance gains justify using the entire dataset? What trade-offs between accuracy and computation time did you notice?\n",
        "\n",
        "#### Your Answer Here:\n",
        "\n",
        "As dataset size increased, the validation metrics showed only a marginal improvement, about 0.003. This suggests that using the entire dataset does not yield practically significant performance gains. Interestingly, computation time slightly decreased as the dataset grew. Overall, although the accuracy improvement is minimal, the fact that computation time also decreased means there is no practical downside to using the entire dataset. The trade-off here is better performance with slightly faster computation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e8c9212-dde0-4a33-b926-70ef9dddd1be",
      "metadata": {
        "id": "1e8c9212-dde0-4a33-b926-70ef9dddd1be"
      },
      "source": [
        "# Problem 3 — Model swap: speed vs. accuracy (why: capacity matters)\n",
        "\n",
        "In this problem we will compare encoder-only backbones of different sizes.\n",
        "\n",
        "**Setup.** Keep the best `MAX_LEN`, `LR`, and `SUBSET_FRAC` from Problems 1–2. Only change the model/preset:\n",
        "\n",
        "* **DistilBERT** (current baseline)\n",
        "* **BERT-base** (larger/usually stronger)\n",
        "\n",
        "**How to switch (two lines each).**\n",
        "\n",
        "* DistilBERT:\n",
        "\n",
        "  ```python\n",
        "  preproc = kh.models.DistilBertTextClassifierPreprocessor.from_preset(\"distil_bert_base_en_uncased\", sequence_length=MAX_LEN)\n",
        "  model  = kh.models.DistilBertTextClassifier.from_preset(\"distil_bert_base_en_uncased\", num_classes=2, preprocessor=preproc)\n",
        "  ```\n",
        "\n",
        "* BERT-base:\n",
        "\n",
        "  ```python\n",
        "  preproc = kh.models.BertTextClassifierPreprocessor.from_preset(\"bert_base_en_uncased\", sequence_length=MAX_LEN)\n",
        "  model  = kh.models.BertTextClassifier.from_preset(\"bert_base_en_uncased\", num_classes=2, preprocessor=preproc)\n",
        "  ```\n",
        "\n",
        "**What to do.**\n",
        "\n",
        "1. Train/evaluate each model once with identical settings.\n",
        "2. Observe the performance metrics for each.\n",
        "3. Answer the graded questions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT"
      ],
      "metadata": {
        "id": "JOT2SVLDPrhC"
      },
      "id": "JOT2SVLDPrhC"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "0ce5521c-e487-4ee6-8ce8-a93a5231eec9",
      "metadata": {
        "id": "0ce5521c-e487-4ee6-8ce8-a93a5231eec9",
        "outputId": "aaec7e8a-79fa-43f2-9d07-e43e1e3614bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m3282/3282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 72ms/step - acc: 0.9078 - loss: 0.2315 - val_acc: 0.9352 - val_loss: 0.1795\n",
            "Epoch 2/3\n",
            "\u001b[1m3282/3282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 56ms/step - acc: 0.9566 - loss: 0.1223 - val_acc: 0.9339 - val_loss: 0.2029\n",
            "Epoch 3/3\n",
            "\u001b[1m3282/3282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 56ms/step - acc: 0.9790 - loss: 0.0652 - val_acc: 0.9328 - val_loss: 0.2444\n",
            "\n",
            "Validation acc (best epoch): 0.935\n",
            "\n",
            "Test accuracy: 0.933   Test F1: 0.933\n",
            "\n",
            "Confusion matrix:\n",
            " [[3476  274]\n",
            " [ 228 3522]]\n",
            "\n",
            "Elapsed time: 00:11:54\n"
          ]
        }
      ],
      "source": [
        "# Your code here; add as many cells as you wish\n",
        "\n",
        "# ---- Keras Hub preprocessor + classifier ----\n",
        "preproc = kh.models.BertTextClassifierPreprocessor.from_preset(\n",
        "    \"bert_base_en_uncased\", sequence_length=MAX_LEN\n",
        ")\n",
        "model = kh.models.BertTextClassifier.from_preset(\n",
        "    \"bert_base_en_uncased\", num_classes=2, preprocessor=preproc\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(7.5e-6),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
        ")\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# ---- Train with early stopping (restore best val weights) ----\n",
        "cb = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)]\n",
        "history = model.fit(\n",
        "    X_tr, y_tr,\n",
        "    validation_data=(X_va, y_va),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH,\n",
        "    callbacks=cb,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# ---- Evaluate (accuracy + F1 via `evaluate`) ----\n",
        "logits = model.predict(X_te, batch_size=EVAL_BATCH, verbose=0)\n",
        "y_pred = logits.argmax(axis=-1)\n",
        "\n",
        "acc_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric  = evaluate.load(\"f1\")\n",
        "acc = acc_metric.compute(predictions=y_pred, references=y_te)[\"accuracy\"]\n",
        "f1  = f1_metric.compute(predictions=y_pred, references=y_te)[\"f1\"]\n",
        "\n",
        "# Tiny confusion matrix helper (no sklearn needed)\n",
        "def confusion_matrix_np(y_true, y_pred, num_classes=2):\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
        "    for t, p in zip(y_true, y_pred):\n",
        "        cm[t, p] += 1\n",
        "    return cm\n",
        "\n",
        "print(f\"\\nValidation acc (best epoch): {history.history['val_acc'][np.argmin(history.history['val_loss'])]:.3f}\")\n",
        "print(f\"\\nTest accuracy: {acc:.3f}   Test F1: {f1:.3f}\")\n",
        "print(\"\\nConfusion matrix:\\n\", confusion_matrix_np(y_te, y_pred))\n",
        "\n",
        "end = time.time() - start\n",
        "print(\"\\nElapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(end)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "147f3698-11a9-4451-a53e-a2e92e18421a",
      "metadata": {
        "id": "147f3698-11a9-4451-a53e-a2e92e18421a"
      },
      "source": [
        "### Graded Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "18d7925a-2494-4689-b329-0b625827a0a8",
      "metadata": {
        "id": "18d7925a-2494-4689-b329-0b625827a0a8"
      },
      "outputs": [],
      "source": [
        "# Set a1a to the validation accuracy at min validation loss for your best model found in this problem\n",
        "\n",
        "a3a = 0.935             # Replace 0.0 with your answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "deec774a-2360-494d-9e2f-989164095a79",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deec774a-2360-494d-9e2f-989164095a79",
        "outputId": "63fbff01-1f3d-417d-cc40-a07b009391b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a3a = 0.9350\n"
          ]
        }
      ],
      "source": [
        "# Graded Answer\n",
        "# DO NOT change this cell in any way\n",
        "\n",
        "print(f'a3a = {a3a:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58b6ce4d-28a7-4414-8d1c-114a6bc5fa5f",
      "metadata": {
        "id": "58b6ce4d-28a7-4414-8d1c-114a6bc5fa5f"
      },
      "source": [
        "#### Question a3b:\n",
        "\n",
        "**Answer briefly.**\n",
        "\n",
        "* Which model gives the best **accuracy/F1**?\n",
        "* Which is **fastest** per epoch?\n",
        "* Given limited development time or compute resources, which model is the best **overall choice** and why?\n",
        "\n",
        "#### Your Answer Here:\n",
        "\n",
        "The BERT model gives the best accuracy/F1. However, the DistilBERT was the fastest per epoch. Given limited development time or compute resources, the DistilBERT model would be the best overall choice. The difference between the metrics of both models was just 0.006, which is not significant enough to make BERT a better model, especially since it took double the time for it to run. Thus, the DistilBERT model is better since it runs faster, even though the accuracy was just slightly worse."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python (tf_env)",
      "language": "python",
      "name": "tf_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}